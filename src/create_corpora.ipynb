{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Corpora\n",
    "\n",
    "(C) 2023 by [Damir Cavar](http://damir.cavar.me/)\n",
    "\n",
    "This notebook generates the datasets for the Ellipsis experiments with Large Language Models (LLMs) and other NLP tools.\n",
    "\n",
    "This code is part of the [NLP-Lab](http://nlp-lab.org/) [Ellipsis Project](https://nlp-lab.org/ellipsis/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "from ipydatagrid import DataGrid\n",
    "import langcodes\n",
    "import csv\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_entry = re.compile(r\"\\n+(?P<ellipsis>.+)\\n^----\\n(?P<fullform>^.+)\\n(?P<rest>(^([AB]:\\s+.+|#.+)\\n)*)\", re.MULTILINE |re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_txt(filename: str):\n",
    "    ellipsis = []\n",
    "    fullforms = []\n",
    "    with open(filename, mode='r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "        counter = 0\n",
    "        for match in re_entry.finditer(text):\n",
    "            # print(match.group('ellipsis'))\n",
    "            ellipsis.append(match.group('ellipsis').strip())\n",
    "            # print(match.group('fullform'))\n",
    "            fullforms.append(match.group('fullform').strip())\n",
    "            #print(match.group('rest'))\n",
    "            counter += 1\n",
    "    return counter, ellipsis, fullforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_txt_format(filename: str, debug: bool=False) -> list:\n",
    "    res = []\n",
    "    if \"ellipsis\" in filename:\n",
    "        with open(filename, mode='r', encoding='utf-8') as ifp:\n",
    "            text = ifp.read()\n",
    "        if debug:\n",
    "            print(text)\n",
    "        counter = 0\n",
    "        for match in re_entry.finditer(text):\n",
    "            #s = match.start()\n",
    "            #e = match.end()\n",
    "            counter += 1\n",
    "        return counter\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"corpus\"\n",
    "\n",
    "langdirs = [ x for x in os.listdir(directory) if len(x) == 3 ]\n",
    "res = []\n",
    "res_data = set()\n",
    "for lfolder in langdirs:\n",
    "    lang = langcodes.Language.get(lfolder).display_name()\n",
    "    if lfolder == \"chn\":\n",
    "        lang = \"Mandarin Chinese\"\n",
    "    if lang.startswith(\"Unknown\"): lang = lfolder\n",
    "    for subdir, dirs, files in os.walk(os.path.join(directory, lfolder)):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                full_path = os.path.join(subdir, file)\n",
    "                DEBUG=False\n",
    "                count, ellipsis, fullform = parse_txt(full_path) # , debug=DEBUG)\n",
    "                if count > 0:\n",
    "                    res.append( (lang, os.path.splitext(os.path.basename(full_path))[0], count) )\n",
    "                if ellipsis:\n",
    "                    res_data.update( set( [ (lang, ) + e for e in zip(ellipsis, fullform) ]) )\n",
    "            elif file.endswith(\".xml\"):\n",
    "                pass\n",
    "res.sort(key=lambda x: x[2], reverse=True)\n",
    "df = pd.DataFrame(res, columns=(\"lang\", \"type\", \"count\"), index=None)\n",
    "bylang = {}\n",
    "for r in res:\n",
    "    bylang[r[0]] = bylang.get(r[0], 0) + r[2]\n",
    "bylang = list(bylang.items())\n",
    "bylang.sort(key=lambda x: x[1], reverse=True)\n",
    "df_bylang = pd.DataFrame(bylang, columns=(\"lang\", \"count\"), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_pairs = [ (TreebankWordDetokenizer().detokenize(x[1].split()), TreebankWordDetokenizer().detokenize(x[2].split())) for x in res_data if x[0] == 'English' ]\n",
    "#for e in eng_pairs:\n",
    "#    print(e)\n",
    "eng_pairs_labeled = [ (TreebankWordDetokenizer().detokenize(e[0].split()), 1) for e in eng_pairs ]\n",
    "eng_pairs_labeled.extend( [ (TreebankWordDetokenizer().detokenize(e[1].split()), 0) for e in eng_pairs ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_distractors = []\n",
    "with open(\"treebank_sentences_no_ellipsis.txt\", mode='r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"#\"):\n",
    "            if line.startswith(\"# STOP\"):\n",
    "                break\n",
    "            continue\n",
    "        eng_pairs_labeled.append( (line, 0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(eng_pairs_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('If I need to ask for money up front later, I will ___.', 1)\n",
      "('Big cyclical companies are using \"all the tricks they can ___ to stabilize earnings,\" says Mr. Sloan.', 1)\n",
      "(\"Jill likes your story even though she hates Bill's ___.\", 1)\n",
      "(\"Many are quick to emphasize that just because the market can fall as fast as it did ___ Friday doesn't mean it will tank again, despite some disquieting similarities between now and October 1987.\", 1)\n",
      "('His wife also works for the paper, as did his father ___.', 1)\n",
      "('Did Frank get married first, or ___ Larry ___?', 1)\n",
      "(\"They didn't ___.\", 1)\n",
      "('With her keen on him, and him ___ on her, the party should be fun.', 1)\n",
      "('\"A lot of people think I will give away the store, but I can assure you I will not ___,\" he says.', 1)\n",
      "('Why, she tried to learn you your book, she tried to learn you your manners, she tried to be good to you every way she knowed how ___.', 1)\n",
      "Examples with ellipsis 559\n",
      "Examples without ellipsis 646\n"
     ]
    }
   ],
   "source": [
    "for x in eng_pairs_labeled[:10]:\n",
    "    print(x)\n",
    "count_ellipsis = sum( [ 1 for x in eng_pairs_labeled if x[1] == 1 ] )\n",
    "count_non_ellipsis = sum( [ 1 for x in eng_pairs_labeled if x[1] == 0 ] )\n",
    "print(f\"Examples with ellipsis {count_ellipsis}\\nExamples without ellipsis {count_non_ellipsis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):\n",
    "    res = re.sub(r\"\\s\\s+\", \" \", s.replace(\"___\", \"\"))\n",
    "    return TreebankWordDetokenizer().detokenize(res.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               sentence  ellipsis\n",
      "0     If I need to ask for money up front later, I w...         1\n",
      "1     Big cyclical companies are using \"all the tric...         1\n",
      "2     Jill likes your story even though she hates Bi...         1\n",
      "3     Many are quick to emphasize that just because ...         1\n",
      "4     His wife also works for the paper, as did his ...         1\n",
      "...                                                 ...       ...\n",
      "1200  Exports in October stood at $5.29 billion, a m...         0\n",
      "1201  South Korea's economic boom, which began in 19...         0\n",
      "1202  Government officials said exports at the end o...         0\n",
      "1203  Despite the gloomy forecast, South Korea has r...         0\n",
      "1204  From January to October, the nation's accumula...         0\n",
      "\n",
      "[1205 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "clean_eng_pairs_labeled = [ (clean_string(x[0]), x[1]) for x in eng_pairs_labeled ]\n",
    "first_task_data = pd.DataFrame(clean_eng_pairs_labeled, columns=[\"sentence\", \"ellipsis\"])\n",
    "print(first_task_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    first_task_data = first_task_data.sample(frac=1).reset_index(drop=True)\n",
    "    first_task_data.to_csv(f'task1_random_{i+1}.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_task_data = pd.DataFrame([ (clean_string(x[0]), x[0]) for x in eng_pairs ], columns=[\"without\", \"with\"])\n",
    "# print(second_task_data[:3])\n",
    "for i in range(10):\n",
    "    second_task_data = second_task_data.sample(frac=1).reset_index(drop=True)\n",
    "    second_task_data.to_csv(f'task2_random_{i+1}.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_task_data = pd.DataFrame([ (clean_string(x[0]), x[1]) for x in eng_pairs ], columns=[\"without\", \"with\"])\n",
    "for i in range(10):\n",
    "    third_task_data = third_task_data.sample(frac=1).reset_index(drop=True)\n",
    "    third_task_data.to_csv(f'task3_random_{i+1}.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert ELLie to IEL Format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ellie(fname):\n",
    "    data = []\n",
    "    with open(fname, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader, None) # skip header\n",
    "        columns = [\"ID\", \"Condition\", \"Sentence\", \"Construction\", \"Semantic_Role\"]\n",
    "        data = list(reader)\n",
    "    return [ (x[2], 1) for x in data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The photographer used the camera, and the reporter did too.', 1), ('The photographer used the camera, and the butcher did too.', 1)]\n"
     ]
    }
   ],
   "source": [
    "ellie_file = os.path.join(\".\", \"corpus\", \"eng\", \"ellipsis\", \"ELLie.csv\")\n",
    "ellie_data = read_ellie(ellie_file)\n",
    "print(ellie_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellie_task_data = pd.DataFrame([ (clean_string(x[0]), x[1]) for x in ellie_data ], columns=[\"sentence\", \"ellipsis\"])\n",
    "for i in range(10):\n",
    "    ellie_task_data = ellie_task_data.sample(frac=1).reset_index(drop=True)\n",
    "    ellie_task_data.to_csv(f'task1_ellie_{i+1}.csv', header=None, index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
